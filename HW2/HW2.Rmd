---
title: "HW2"
author: "Anish mohan"
date: "September 29, 2015"
output: html_document
---
1. Q1
  + 1a.  Proof
  
  + 1b. As proved, the objective function $\sum_{i,i' \in C_{k}} \sum_{j=1}^{p}\{{x_{i,j}-x_{i'j}}\}^{2}$ is equivalent to finding the sum of distances of the point from the centroid of the cluster. Now, during each iteration each point is assigned to the closest centroid, hence in each iteration the cluster of points in a class are getting closer to the centroid of the class (obtained by current set of points of class). The process continues in each iteration and we continue to reduce the distance between points that belong to the same class.
 
 + 1c.  Flexible model will generally perform better

  
 + 1d. Inflexible model will generally perform better.

2. Q2:
  + 2a. Regression. Inference.

  + 2b. Classification. Prediction

  + 2c. AB and CD are the two clusters
  
  + 2d. ABC and D are the two clusters
  
  + 2e.
         
3. Q3

  + 3a.
```{r}
Arrests_Data=USArrests
HC_Arrests_Complete=hclust(dist(Arrests_Data),method="complete")
plot(HC_Arrests_Complete, main ="US Arrests-Complete Linkage",xlab="States", sub=" ",cex=0.9)
```

  + 3b. 
```{r}
cutree(HC_Arrests_Complete,3)
```

  + 3c.
```{r}
Arrests_Data_Scaled=scale(Arrests_Data,scale=TRUE)
HC_ScaledArrests_Complete=hclust(dist(Arrests_Data_Scaled),method="complete")
plot(HC_ScaledArrests_Complete, main ="US Scaled Arrests-Complete Linkage",xlab="States", sub="",cex=0.9)
```

  + 3d.
    Scaling the variables has the clustering of the states and the clustering after states is different before and after 
    scaling. For e.g Arizon and Arkansas have moved to different clusters after scaling.
    
    Scaling should be done before creating the distance/dissimilarity matrix and some variables/features have higher values 
    e.g Assault, that overwhelms the results from variables/feature with lower values/range e.g Murder in the USArrests 
    Data.
    

4. Q4
  + 4a. Theoretically, it is possible to have the linear regression and cubic regression to have the same or similar RSS if the true relationship is linear. The regression model for cubic (when the underlying model is linear) should give us $\beta_{2}$ and $\beta_{3}$ == 0. 
    However, since the the training data would contain noise and a cubic model would be more prone to fitting the noise, the RSS value is expected to be lower than that for linear regression model.
        
  + 4b. Test data will contain noise and the cubic model will be more prone to noise. The cubic model being more flexible 
        will fit to the noise in the data and will have higher residual error than linear model with real datasets.
  
  + 4c. If the true relationship is not linear then the accuracy of the model will depend upon the noise in the data and the         amount of non-linearity.
  
    In general, a cubic regression model (flexible) would perform better than a linear regression model when the underlying 
    function is non-linear. RSS error on training data should be lower with the cubic regression model.
  
    Linear regression model introduces bias when used for non-linear true function hence can result in more errors.
  
    Noise in the data can impact the results we get from a cubic model. Noisy data can cause the variance to be high and 
    impact the results from a cubic model as the model is flexible and prone to overfitting to noise.
  
  + 4d. Same as above. In general, it is difficult to give an estimation of errors without knowing the true function, however for most scenarios (with low noise) cubic model should perform better with test data if the underlying model is non linear.
  
5. Q5
+ 5a.
    ```{r}
  library(MASS)
  library(ISLR)
    autodat=Auto
    pairs(autodat)
    ```
+ 5b.
    ```{r}
    cor(autodat[,1:8])
    ```
    
+ 5c.
    ```{r}
    attach(autodat)
    autolm=lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin)
    summary(autolm)
    ```
    + i.
      Yes there is relationship between some of the predictors and the response (mpg) as can be seen from the graph. 
      Example, there is a correlation between mpg and displacement, mpg and horsepower, mpg and weight etc.
    
    + ii. From the summary table, Year, Weight,Origin seem to have statistically significant (p<0.001) relationship to MPG.
    
    + iii. Coefficient for the year variable is 0.75, hence it suggest that given specific values for other predictors,     
            every year the MPG increases by 0.75 unit 
      
+ 5d. 
    ```{r}
    par(mfrow=c(2,2))
    plot(autolm)
    ```
    
    + The Residuals vs Fitted plot shows a trend line and the shape of the trend line suggests non-linearity in the data.
    + Some points #321,#324 in Residuals vs Fitted graph, have higher residual values and they potentially could be the 
      outliers.
    + Point #14 in Residuals vs Leverage Graph, has high leverage.
    
    
6. Q6

  + 6a. 
    Equation: $y=2+2*x_{1}+ 0.3*x_{2}$
    
    $\beta_{0}=2,$
    $\beta_{1}=2,$
    $\beta_{2}=0.3$
  
  + 6b. 
  
```{r}
    set.seed(1)
    x1=runif(100)
    x2=0.5*x1+rnorm(100)/10
    y=2+2*x1+0.3*x2+rnorm(100)
    cor(x1,x2)
    plot(x2,x2)
```

+ 6c. 
```{r}
    ylm=lm(y~x1+x2)
    summary(ylm)
```
  + $\hat{\beta_{0}}=2.13$ 
    $\hat{\beta_{0}}$ is a good estimator of $\beta_{0}$ as the t-value is high and low p-value.
  
  + $\hat{\beta_{1}}=1.4396$ 
    $\hat{\beta_{1}}$ provides reasonable estimate of $\beta_{1}$ and gets close enough. This indicated by comparitively a 
    t-statistic that is not very high and p-value that is near the cut-off of 0.05.
  
  + $\hat{\beta_{2}}=1.0097$
    $\hat{\beta_{1}}$ is a poor estimator of $\beta_{1}$. t-statistic is fairly low and p-value is high
  
  + Yes, $H_{0}: \beta_{1} =0$ can be rejected as p-value =0.04 is below the cut-off of 0.05 or 5%
  
  + No, $H_{0}: \beta_{2} =0$ cannot be rejected as p-value =0.375 is above the cut-off of 0.05 or 5%

+ 6d. 
```{r}
    y2lm=lm(y~x1)
    summary(y2lm)
```
  +  Yes, $H_{0}: \beta_{1} =0$ can be rejected as p-value well below 0.001

+ 6e. 
```{r}
    y3lm=lm(y~x2)
    summary(y3lm)
```
  +  Yes, $H_{0}: \beta_{2} =0$ can be rejected as p-value well below 0.001

+ 6f.
    No, the results do not contradict each other. Both $x_{1}$ and $x_{2}$ individually are good predictors of y. That is 
    shown by 6d and 6e. However, $x_{1}$ and $x_{2}$ are highly correlated. Hence once $\beta_{1}$ provided appropirate 
    weighting to $x_{1}$, adding $x_{2}$ does not introduce any additional information for better fit of y.
  
+6g. 
```{r}
