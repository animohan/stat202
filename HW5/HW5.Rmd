---
title: "HW5"
author: "Anish Mohan"
date: "November 1, 2015"
output: html_document
---

# Q1
  + 1a. Training RSS will start decreasing. 
    $\beta_{j}$'s start increasing from 0 to s, hence the value of the training RSS will start decreasing as the $\beta_{j}$'s get to their correct values.
    
  + 1b. Test RSS will decrease initially and then increase.
    Test RSS will decrease as $\beta_{j}$'s increase from 0. After a local minima that gives the best value for $\beta_{j}$'s the Test RSS will start increasing as the the $\beta_{j}$'s are determined from the training set.
    
  + 1c. variance starts increasing
  $\beta_{j}$'s=0 has a constant low variance independent of the data. Variance starts increasing as the s increases from 0.
  
  + 1d. bias starts decreasing
  $\beta_{j}$'s=0 has the highest bias as the model predicts a constant value. As s increases from 0, the bias will start decreasing.
  
  + 1e. Irreducible error remains steady
  Irreducible by error cannot be determined and continues to stay steady.
  

# Q2
    + 2a. Training RSS will start increasing 
    With $\lambda$=0, the solution is what get from oridinary least squares that minimize the training MSE. As $\lambda$ starts increasing from 0, the training error will start increasing as well.
    
  + 2b. Test RSS will decrease initially and then increase.
    Test RSS will initially decrease as $\lambda$ increases from 0 as the $\beta_{j}$'s predicted from training set are able to predict value of the test set with error of margin. However after a certain point that models the best $lambda$ and $beta_{j}$'s for the test set, the test RSS will start going up. 

  + 2c. variance starts decreasing
  $\lambda$=0 gives the least squares solution. As $\lambda$ starts increasing the flexibility of the model starts decreasing and the variance of the model starts decreasing as well.

  
  + 2d. bias starts increasing
  $\lambda$=0 gives the least squares solution. As $\lambda$ starts increasing the flexibility of the model starts decreasing and the bias of the model starts increasing as well.
  
  + 2e. Irreducible error remains steady
  Irreducible by error cannot be determined and continues to stay steady.

# Q3 
  + 3a. For k, predictors, the best subset will have the smallest training RSS, because it looks at all k subsets and chooses the subset with lowest RSS.
  
  + 3b. Cannot be reliably predicted and depends on the test data. Best-subset overfits to training data so if it captures the underlying model then the lowest test RSS could be through Best subset. However, forward and backward stepwise selections could also have the least test RSS.
  
  + 3c.
      + i. True
        Forward stepwise is incremental and k+1 the iteration contains all variables of kth iterarion and an additonal variable.
        
      + ii.  True
        Backward stepwise removes one element in each iteration. So kth iteration will have 1 less variable than in k+1 iteration
      
      + iii. False
        It is not guaranteed to happen.
      
      + iv. False
        It is not guaranteed to happen.
      
      + v. False
      K+1 iteration could have elements not in kth iteration.
      
#Q4 
  + 4a
    ```{r}
      set.seed(1)
      X=rnorm(100)
      eps=rnorm(100)

    ```

  + 4b
    ```{r}
      X2=X^2
      X3=X^3
      #beta0=1
      #beta1=2
      #beta2=-3
      #beta3=0.3
      
      beta0=1
      beta1=1
      beta2=1
      beta3=1
      Y=beta0+beta1*X+beta2*X2+beta3*X3+eps
    ```

  + 4c
    ```{r}
      library(leaps)
      df=data.frame(y=Y,x=X)
      regfit.X=regsubsets(y~poly(x,10,raw=T), data=df, nvmax=10)
      regfitx.summary=summary(regfit.X)

      par(mfrow=c(2,2))
      
      plot(regfitx.summary$bic, xlab="Number of variables", ylab="bic",type = "l")
      k=which.min(regfitx.summary$bic)
      points(k,regfitx.summary$bic[k],col="red",cex=2,pch=20)
      
      plot(regfitx.summary$adjr2, xlab="Number of variables", ylab="Adjusted RSq",type = "l")
      k=which.max(regfitx.summary$adjr2)
      points(k,regfitx.summary$adjr2[k],col="red",cex=2,pch=20)
      
      plot(regfitx.summary$cp, xlab="Number of variables", ylab="Cp",type="l")
      k=which.min(regfitx.summary$cp)
      points(k,regfitx.summary$cp[k],col="red",cex=2,pch=20)
      
      coefficients(regfit.X, id=3)
      coefficients(regfit.X, id=4)
    ```


  +4d.
    ```{r}
      regfit.fwd=regsubsets(y~poly(x,10,raw=T), data=df, nvmax=10, method="forward")
      regfitfwd.summary=summary(regfit.X)

      par(mfrow=c(2,2))
      
      plot(regfitfwd.summary$bic, xlab="Number of variables", ylab="bic",type = "l")
      k=which.min(regfitfwd.summary$bic)
      k
      points(k,regfitfwd.summary$bic[k],col="red",cex=2,pch=20)
      
      plot(regfitfwd.summary$adjr2, xlab="Number of variables", ylab="Adjusted RSq",type = "l")
      k=which.max(regfitfwd.summary$adjr2)
      k
      points(k,regfitfwd.summary$adjr2[k],col="red",cex=2,pch=20)
      
      plot(regfitfwd.summary$cp, xlab="Number of variables", ylab="Cp",type="l")
      k=which.min(regfitfwd.summary$cp)
      k
      points(k,regfitfwd.summary$cp[k],col="red",cex=2,pch=20)
      
      coefficients(regfit.fwd, id=3)
      coefficients(regfit.fwd, id=4) 
      
      
      #Backward
      regfit.bwd=regsubsets(y~poly(x,10,raw=T), data=df, nvmax=10, method="backward")
      regfitbwd.summary=summary(regfit.X)

      par(mfrow=c(2,2))
      
      plot(regfitbwd.summary$bic, xlab="Number of variables", ylab="bic",type = "l")
      k=which.min(regfitbwd.summary$bic)
      k
      points(k,regfitbwd.summary$bic[k],col="red",cex=2,pch=20)
      
      plot(regfitbwd.summary$adjr2, xlab="Number of variables", ylab="Adjusted RSq",type = "l")
      k=which.max(regfitbwd.summary$adjr2)
      k
      points(k,regfitbwd.summary$adjr2[k],col="red",cex=2,pch=20)
      
      plot(regfitbwd.summary$cp, xlab="Number of variables", ylab="Cp",type="l")
      k=which.min(regfitbwd.summary$cp)
      k
      points(k,regfitbwd.summary$cp[k],col="red",cex=2,pch=20)
      
      coefficients(regfit.bwd, id=3)
      coefficients(regfit.bwd, id=4)
      
    ```

      Statistics from Forward and Backward models show 3 and 4 variable models are optimal. This is similar to results in 4c.
      
      