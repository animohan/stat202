---
title: "HW5"
author: "Anish Mohan"
date: "November 1, 2015"
output: html_document
---

# Q1
  + 1a. Training RSS will start decreasing. 
    $\beta_{j}$'s start increasing from 0 to s, hence the value of the training RSS will start decreasing as the $\beta_{j}$'s get to their correct values.
    
  + 1b. Test RSS will decrease initially and then increase.
    Test RSS will decrease as $\beta_{j}$'s increase from 0. After a local minima that gives the best value for $\beta_{j}$'s the Test RSS will start increasing as the the $\beta_{j}$'s are determined from the training set.
    
  + 1c. variance starts increasing
  $\beta_{j}$'s=0 has a constant low variance independent of the data. Variance starts increasing as the s increases from 0.
  
  + 1d. bias starts decreasing
  $\beta_{j}$'s=0 has the highest bias as the model predicts a constant value. As s increases from 0, the bias will start decreasing.
  
  + 1e. Irreducible error remains steady
  Irreducible by error cannot be determined and continues to stay steady.
  

# Q2
    + 2a. Training RSS will start increasing 
    With $\lambda$=0, the solution is what get from oridinary least squares that minimize the training MSE. As $\lambda$ starts increasing from 0, the training error will start increasing as well.
    
  + 2b. Test RSS will decrease initially and then increase.
    Test RSS will initially decrease as $\lambda$ increases from 0 as the $\beta_{j}$'s predicted from training set are able to predict value of the test set with error of margin. However after a certain point that models the best $lambda$ and $beta_{j}$'s for the test set, the test RSS will start going up. 

  + 2c. variance starts decreasing
  $\lambda$=0 gives the least squares solution. As $\lambda$ starts increasing the flexibility of the model starts decreasing and the variance of the model starts decreasing as well.

  
  + 2d. bias starts increasing
  $\lambda$=0 gives the least squares solution. As $\lambda$ starts increasing the flexibility of the model starts decreasing and the bias of the model starts increasing as well.
  
  + 2e. Irreducible error remains steady
  Irreducible by error cannot be determined and continues to stay steady.

# Q3 
  + 3a. For k, predictors, the best subset will have the smallest training RSS, because it looks at all k subsets and chooses the subset with lowest RSS.
  
  + 3b. Cannot be reliably predicted and depends on the test data. Best-subset overfits to training data so if it captures the underlying model then the lowest test RSS could be through Best subset. However, forward and backward stepwise selections could also have the least test RSS.
  
  + 3c.
      + i. True
        Forward stepwise is incremental and k+1 the iteration contains all variables of kth iterarion and an additonal variable.
        
      + ii.  True
        Backward stepwise removes one element in each iteration. So kth iteration will have 1 less variable than in k+1 iteration
      
      + iii. False
        It is not guaranteed to happen.
      
      + iv. False
        It is not guaranteed to happen.
      
      + v. False
      K+1 iteration could have elements not in kth iteration.
      
#Q4 
  + 4a
    ```{r}
      set.seed(1)
      X=rnorm(100)
      eps=rnorm(100)

    ```

  + 4b
    ```{r}
      X2=X^2
      X3=X^3

      beta0=1
      beta1=1
      beta2=1
      beta3=1
      Y=beta0+beta1*X+beta2*X2+beta3*X3+eps
    ```

  + 4c
  
    ```{r}
      library(leaps)
      df=data.frame(y=Y,x=X)
      regfit.X=regsubsets(y~poly(x,10,raw=T), data=df, nvmax=10)
      regfitx.summary=summary(regfit.X)

      par(mfrow=c(2,2))
      
      plot(regfitx.summary$bic, xlab="Number of variables", ylab="bic",type = "l")
      k=which.min(regfitx.summary$bic)
      points(k,regfitx.summary$bic[k],col="red",cex=2,pch=20)
      
      plot(regfitx.summary$adjr2, xlab="Number of variables", ylab="Adjusted RSq",type = "l")
      k=which.max(regfitx.summary$adjr2)
      points(k,regfitx.summary$adjr2[k],col="red",cex=2,pch=20)
      
      plot(regfitx.summary$cp, xlab="Number of variables", ylab="Cp",type="l")
      k=which.min(regfitx.summary$cp)
      points(k,regfitx.summary$cp[k],col="red",cex=2,pch=20)
      
      coefficients(regfit.X,3)
      coefficients(regfit.X,4)
    ```
    
    3 variable model picks X, X^2 and X^3
    4 variable model picks X, X^2, X^3 and X^5
      
  + 4d.
  
    ```{r}
      regfit.fwd=regsubsets(y~poly(x,10,raw=T), data=df, nvmax=10, method="forward")
      regfitfwd.summary=summary(regfit.X)

      par(mfrow=c(2,2))
      
      plot(regfitfwd.summary$bic, xlab="Number of variables", ylab="bic",type = "l")
      k=which.min(regfitfwd.summary$bic)
      points(k,regfitfwd.summary$bic[k],col="red",cex=2,pch=20)
      
      plot(regfitfwd.summary$adjr2, xlab="Number of variables", ylab="Adjusted RSq",type = "l")
      k=which.max(regfitfwd.summary$adjr2)
      points(k,regfitfwd.summary$adjr2[k],col="red",cex=2,pch=20)
      
      plot(regfitfwd.summary$cp, xlab="Number of variables", ylab="Cp",type="l")
      k=which.min(regfitfwd.summary$cp)
      points(k,regfitfwd.summary$cp[k],col="red",cex=2,pch=20)
      
      coefficients(regfit.fwd,3)
      coefficients(regfit.fwd,4) 
    ```
     
     
    ```{r}  
      #Backward
      regfit.bwd=regsubsets(y~poly(x,10,raw=T), data=df, nvmax=10, method="backward")
      regfitbwd.summary=summary(regfit.X)

      par(mfrow=c(2,2))
      plot(regfitbwd.summary$bic, xlab="Number of variables", ylab="bic",type = "l")
      k=which.min(regfitbwd.summary$bic)
      k
      points(k,regfitbwd.summary$bic[k],col="red",cex=2,pch=20)
      
      plot(regfitbwd.summary$adjr2, xlab="Number of variables", ylab="Adjusted RSq",type = "l")
      k=which.max(regfitbwd.summary$adjr2)
      k
      points(k,regfitbwd.summary$adjr2[k],col="red",cex=2,pch=20)
      
      plot(regfitbwd.summary$cp, xlab="Number of variables", ylab="Cp",type="l")
      k=which.min(regfitbwd.summary$cp)
      k
      points(k,regfitbwd.summary$cp[k],col="red",cex=2,pch=20)
      
      coefficients(regfit.bwd,3)
      coefficients(regfit.bwd,4)
      
    ```

      Statistics from Forward and Backward models show 3 and 4 variable models are optimal. Additionally,3 variable model picks X, X^2 and X^3 and 4 variable model picks X, X^2, X^3 and X^5. 
      These results are similar to results in 4c.

  + 4e
  
    ```{r}
      par(mfrow=c(1,1))
      library(glmnet)
      xnew=model.matrix(y~poly(x,10,raw=T),data=df)[,-1]
      grid=10^seq(10,-2,length=100)
      
      set.seed(1)
      train=sample(1:nrow(xnew),nrow(xnew)/2)
      test=(-train)
      Y.test=Y[test]
      
      lasso.mod=glmnet(xnew[train,],Y[train], alpha=1,lambda=grid)
      plot(lasso.mod)
      
      cv.out=cv.glmnet(xnew[train,], Y[train], alpha=1)
      plot(cv.out)
      bestlam=cv.out$lambda.min
      
      lasso.pred=predict(lasso.mod,s=bestlam,newx=xnew[test,], type="coefficients")
      lasso.pred
    ```
   
   Lasso predicts a model using $X^{1}$,$X^{2}$, $X^{3}$, $X^{4}$, $X^{5}$. All variables except $X^{4}$ were chosen with backward, forward subselection example above.


+ 4f

    ```{r}
    
    set.seed(1)
    beta7 = 1
    Y=beta0+beta7*X^7+eps
    df=data.frame(y=Y,x=X)
    regfitx7=regsubsets(y~poly(x,10,raw=T), data=df, nvmax=10)
    regfitx7.summary=summary(regfitx7)
    k=which.min(regfitx7.summary$bic)
    coefficients(regfitx7,k)
    k=which.min(regfitx7.summary$cp)
    coefficients(regfitx7,k)
    k=which.max(regfitx7.summary$adjr2)
    coefficients(regfitx7,k)
    ```
   
    BIC picks the correct 1 variable model with $X^7$; Cp picks 2 variable model with $X^2$ and $X^7$ and Adjusted $R^2$ picks a 4 variable model with $X^1$, $X^2$,$X^3$ and $X^7$
    
    ```{r}
      xnew=model.matrix(y~poly(x,10,raw=T),data=df)[,-1]
      cv.out=cv.glmnet(xnew, Y, alpha=1)
      bestlam=cv.out$lambda.min
      
      lasso.pred=predict(lasso.mod,s=bestlam,newx=xnew, type="coefficients")
      lasso.pred
    ```
    
    Lasso picks the 2 variable model with $X^3$ and $X^5$. The intercept value is 1.7 as compared to 1.07 in the best subset selection
    

# Q5. 
  
  + 5a.
    ```{r}
    library(ISLR)
    set.seed(1)
    sum(is.na(College))
    train=sample(1:nrow(College),nrow(College)/2)
    test=(-train)
    College.train=College[train,]
    College.test=College[test,]
    ```
 
  + 5b.
    ```{r}
    lm.fit=lm(Apps~.,data=College.train)
    lm.pred=predict(lm.fit, College.test)
    mean((College.test[,"Apps"]-lm.pred)^2)
    ```
    
    RSS= 1108531
    
  + 5c.
    ```{r}
    library(glmnet)
    ridge_train=model.matrix(Apps~.,data=College.train)
    ridge_test=model.matrix(Apps~.,data=College.test)
    grid=10^seq(4,-2,length=100)
    ridge.mod=cv.glmnet(ridge_train, College.train[,"Apps"], alpha=0, lambda=grid)
    bestlam=ridge.mod$lambda.min
    bestlam
    ridge.pred=predict(ridge.mod, newx=ridge_test, s=bestlam)
    mean((College.test[,"Apps"]-ridge.pred)^2)
    ```
  
  RSS= 1108062. The test RSS is comparable to the result from least squares fit.
  
  + 5d.
    ```{r}
    lasso.mod=cv.glmnet(ridge_train, College.train[,"Apps"], alpha=1, lambda=grid)
    bestlam=lasso.mod$lambda.min
    lasso.pred=predict(lasso.mod,newx=ridge_test, s=bestlam)
    mean((College.test[,"Apps"]-lasso.pred)^2)
    predict(lasso.mod,newx=ridge_test, s=bestlam, type="coefficients")
    ```
  
  RSS error (1026783) is lower than ridge and least squares
  
  + 5e.
    ```{r}
    library(pls)
    pcr.fit=pcr(Apps~., data=College, subset=train, scale=T, validation="CV")
    validationplot(pcr.fit, val.type="MSEP")
    summary(pcr.fit)
    pcr.pred=predict(pcr.fit, College.test, ncomp=16)
    mean((College.test[,"Apps"]- data.frame(pcr.pred))^2)
    ```
  
  Test RSS using 16 components (1166897) is higher than lasso, ridge and least squares
  
  + 5f  
    ```{r}
        pls.fit=plsr(Apps~., data=College, subset=train, scale=T, validation="CV")
    validationplot(pls.fit, val.type="MSEP")
    summary(pls.fit)
    pls.pred=predict(pls.fit, College.test, ncomp=14)
    mean((College.test[,"Apps"]- data.frame(pls.pred))^2)
    ```
    
      Test RSS using 14 components (1112475) is higher than lasso, ridge and least squares and smaller than pcr
      
  + 5g. 
  Lasso gave the best results. Fit using Ridge and Least Squares was similar to lasso and the Test RSS was comparable.The Test RSS using Principal components regression and partial least squares were within the range of 10% Test RSS reported by Lasso.
  

# Q6
 + 6a.
 + 6b.
 + 6c.
 + 6d.
 + 6e.
 + 6f.
 + 6g.